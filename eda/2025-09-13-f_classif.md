## using scikit learn to prune features 
Generate some data, and do pruning against both a continuous and discrete binary target.




```python
import numpy as np
from sklearn.feature_selection import f_regression, f_classif, mutual_info_regression
from sklearn.datasets import make_regression
```


```python
from sklearn.feature_selection import SelectKBest
```


```python
import polars as pl
```

### this is the function I wrote a while back before I realized that f_regression takes a numpy array of multiple features actually. 
Kept it to sanity check that they are equivalent


```python
def evaluate_feature(df, feature, target):
    X = df[feature].to_numpy()
    num_rows = X.shape[0]
    X = np.reshape(X, (num_rows, 1))
    y = df[target].to_numpy()
    f_value, _ = f_regression(X, y)
    return f_value
    print(feature, f_value)

```


```python

features = ["c1", "c2", "c3", "c4", "c5"]
X = np.random.random(size=(10000, 5))
df = pl.from_numpy(X, schema=features)
df = df.with_columns(
    (pl.col("c1") + pl.col("c2") + pl.col("c3")).alias("y"),
)
df = df.with_columns(
    ((pl.col("y") >= 2).cast(pl.Int8)).alias("target")
)
df.group_by("target").len()
```




<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 2)</small><table border="1" class="dataframe"><thead><tr><th>target</th><th>len</th></tr><tr><td>i8</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>8337</td></tr><tr><td>1</td><td>1663</td></tr></tbody></table></div>




```python
[
    evaluate_feature(df, feature, 'y')
    for feature in ['y'] + features
]
```




    [array([-3.75224909e+18]),
     array([4935.67950579]),
     array([4823.40828645]),
     array([4946.46681529]),
     array([0.88096144]),
     array([1.8142133])]




```python
f_regression(df.select(features).to_numpy(), df["y"].to_numpy())
```




    (array([4.93567951e+03, 4.82340829e+03, 4.94646682e+03, 8.80961436e-01,
            1.81421330e+00]),
     array([0.        , 0.        , 0.        , 0.34796116, 0.17803416]))



#### showing that f_classif with a continuous target indeed does not work
, since ANOVA is about variance between classes and yea there are no classes in a regression problem


```python
f_classif(df.select(features).to_numpy(), df["y"].to_numpy())
```

    /Users/michal/.python_venvs/uvpy13/lib/python3.13/site-packages/sklearn/feature_selection/_univariate_selection.py:107: RuntimeWarning: invalid value encountered in divide
      msw = sswn / float(dfwn)





    (array([nan, nan, nan, nan, nan]), array([nan, nan, nan, nan, nan]))




```python
f_classif(df.select(features).to_numpy(), df["target"].to_numpy())
```




    (array([1.73154223e+03, 1.78842297e+03, 1.73917650e+03, 3.51022636e-01,
            2.75823180e-01]),
     array([0.        , 0.        , 0.        , 0.55354821, 0.59946366]))



#### however f_regression with a binary target is fine


```python
f_regression(df.select(features).to_numpy(), df["target"].to_numpy())
```




    (array([1.73154223e+03, 1.78842297e+03, 1.73917650e+03, 3.51022636e-01,
            2.75823180e-01]),
     array([0.        , 0.        , 0.        , 0.55354821, 0.59946366]))



## redo using the `dataset.make_regression` 


```python
X, y = make_regression(
     n_samples=10000, n_features=5, n_informative=2, noise=1e-4, random_state=42
)
```


```python
f_statistic, p_values = f_regression(X, y)
f_statistic
```




    array([4.11657999e-02, 1.84012917e+08, 1.33793053e+00, 2.41241615e-01,
           2.75484116e+00])




```python
f_statistic.tolist()
```




    [0.041165799878519746,
     184012916.6745747,
     1.3379305304577502,
     0.24124161482034742,
     2.754841161408661]




```python
pl.from_numpy(X)
```




<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (10_000, 5)</small><table border="1" class="dataframe"><thead><tr><th>column_0</th><th>column_1</th><th>column_2</th><th>column_3</th><th>column_4</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1.491614</td><td>0.50909</td><td>-0.678055</td><td>0.30336</td><td>-0.39035</td></tr><tr><td>0.291727</td><td>0.282634</td><td>-0.444699</td><td>-0.232362</td><td>-0.501386</td></tr><tr><td>-0.086795</td><td>-0.433946</td><td>0.90519</td><td>0.000887</td><td>-0.112914</td></tr><tr><td>0.110207</td><td>-0.053098</td><td>1.156007</td><td>0.028398</td><td>-0.867092</td></tr><tr><td>0.678377</td><td>1.035277</td><td>2.052395</td><td>-1.234422</td><td>-0.020884</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2.710492</td><td>1.631534</td><td>-0.288728</td><td>0.772791</td><td>-1.604054</td></tr><tr><td>0.308692</td><td>-0.647586</td><td>0.424503</td><td>0.113557</td><td>-0.449453</td></tr><tr><td>-0.247086</td><td>1.247497</td><td>1.15459</td><td>-1.003477</td><td>1.132423</td></tr><tr><td>-0.959902</td><td>-0.03244</td><td>1.317566</td><td>-0.833291</td><td>0.152389</td></tr><tr><td>0.927646</td><td>-0.182718</td><td>1.677684</td><td>-0.630445</td><td>0.204154</td></tr></tbody></table></div>


