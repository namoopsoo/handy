```python
import numpy as np
from sklearn.feature_selection import f_regression, f_classif, mutual_info_regression
from sklearn.datasets import make_regression
```


```python
from sklearn.feature_selection import SelectKBest
```


```python
import polars as pl
```


```python


def evaluate_feature(df, feature, target):
    X = df[feature].to_numpy()
    num_rows = X.shape[0]
    X = np.reshape(X, (num_rows, 1))
    y = df[target].to_numpy()
    f_value, _ = f_regression(X, y)
    return f_value
    print(feature, f_value)

```


```python
f_regression?
```


    [31mSignature:[39m f_regression(X, y, *, center=[38;5;28;01mTrue[39;00m, force_finite=[38;5;28;01mTrue[39;00m)
    [31mDocstring:[39m
    Univariate linear regression tests returning F-statistic and p-values.
    
    Quick linear model for testing the effect of a single regressor,
    sequentially for many regressors.
    
    This is done in 2 steps:
    
    1. The cross correlation between each regressor and the target is computed
       using :func:`r_regression` as::
    
           E[(X[:, i] - mean(X[:, i])) * (y - mean(y))] / (std(X[:, i]) * std(y))
    
    2. It is converted to an F score and then to a p-value.
    
    :func:`f_regression` is derived from :func:`r_regression` and will rank
    features in the same order if all the features are positively correlated
    with the target.
    
    Note however that contrary to :func:`f_regression`, :func:`r_regression`
    values lie in [-1, 1] and can thus be negative. :func:`f_regression` is
    therefore recommended as a feature selection criterion to identify
    potentially predictive feature for a downstream classifier, irrespective of
    the sign of the association with the target variable.
    
    Furthermore :func:`f_regression` returns p-values while
    :func:`r_regression` does not.
    
    Read more in the :ref:`User Guide <univariate_feature_selection>`.
    
    Parameters
    ----------
    X : {array-like, sparse matrix} of shape (n_samples, n_features)
        The data matrix.
    
    y : array-like of shape (n_samples,)
        The target vector.
    
    center : bool, default=True
        Whether or not to center the data matrix `X` and the target vector `y`.
        By default, `X` and `y` will be centered.
    
    force_finite : bool, default=True
        Whether or not to force the F-statistics and associated p-values to
        be finite. There are two cases where the F-statistic is expected to not
        be finite:
    
        - when the target `y` or some features in `X` are constant. In this
          case, the Pearson's R correlation is not defined leading to obtain
          `np.nan` values in the F-statistic and p-value. When
          `force_finite=True`, the F-statistic is set to `0.0` and the
          associated p-value is set to `1.0`.
        - when a feature in `X` is perfectly correlated (or
          anti-correlated) with the target `y`. In this case, the F-statistic
          is expected to be `np.inf`. When `force_finite=True`, the F-statistic
          is set to `np.finfo(dtype).max` and the associated p-value is set to
          `0.0`.
    
        .. versionadded:: 1.1
    
    Returns
    -------
    f_statistic : ndarray of shape (n_features,)
        F-statistic for each feature.
    
    p_values : ndarray of shape (n_features,)
        P-values associated with the F-statistic.
    
    See Also
    --------
    r_regression: Pearson's R between label/feature for regression tasks.
    f_classif: ANOVA F-value between label/feature for classification tasks.
    chi2: Chi-squared stats of non-negative features for classification tasks.
    SelectKBest: Select features based on the k highest scores.
    SelectFpr: Select features based on a false positive rate test.
    SelectFdr: Select features based on an estimated false discovery rate.
    SelectFwe: Select features based on family-wise error rate.
    SelectPercentile: Select features based on percentile of the highest
        scores.
    
    Examples
    --------
    >>> from sklearn.datasets import make_regression
    >>> from sklearn.feature_selection import f_regression
    >>> X, y = make_regression(
    ...     n_samples=50, n_features=3, n_informative=1, noise=1e-4, random_state=42
    ... )
    >>> f_statistic, p_values = f_regression(X, y)
    >>> f_statistic
    array([1.21, 2.67e13, 2.66])
    >>> p_values
    array([0.276, 1.54e-283, 0.11])
    [31mFile:[39m      ~/.python_venvs/uvpy13/lib/python3.13/site-packages/sklearn/feature_selection/_univariate_selection.py
    [31mType:[39m      function



```python
f_classif?
```


    [31mSignature:[39m f_classif(X, y)
    [31mDocstring:[39m
    Compute the ANOVA F-value for the provided sample.
    
    Read more in the :ref:`User Guide <univariate_feature_selection>`.
    
    Parameters
    ----------
    X : {array-like, sparse matrix} of shape (n_samples, n_features)
        The set of regressors that will be tested sequentially.
    
    y : array-like of shape (n_samples,)
        The target vector.
    
    Returns
    -------
    f_statistic : ndarray of shape (n_features,)
        F-statistic for each feature.
    
    p_values : ndarray of shape (n_features,)
        P-values associated with the F-statistic.
    
    See Also
    --------
    chi2 : Chi-squared stats of non-negative features for classification tasks.
    f_regression : F-value between label/feature for regression tasks.
    
    Examples
    --------
    >>> from sklearn.datasets import make_classification
    >>> from sklearn.feature_selection import f_classif
    >>> X, y = make_classification(
    ...     n_samples=100, n_features=10, n_informative=2, n_clusters_per_class=1,
    ...     shuffle=False, random_state=42
    ... )
    >>> f_statistic, p_values = f_classif(X, y)
    >>> f_statistic
    array([2.21e+02, 7.02e-01, 1.70e+00, 9.31e-01,
           5.41e+00, 3.25e-01, 4.71e-02, 5.72e-01,
           7.54e-01, 8.90e-02])
    >>> p_values
    array([7.14e-27, 4.04e-01, 1.96e-01, 3.37e-01,
           2.21e-02, 5.70e-01, 8.29e-01, 4.51e-01,
           3.87e-01, 7.66e-01])
    [31mFile:[39m      ~/.python_venvs/uvpy13/lib/python3.13/site-packages/sklearn/feature_selection/_univariate_selection.py
    [31mType:[39m      function



```python

features = ["c1", "c2", "c3", "c4", "c5"]
X = np.random.random(size=(10000, 5))
df = pl.from_numpy(X, schema=features)
df = df.with_columns(
    (pl.col("c1") + pl.col("c2") + pl.col("c3")).alias("y"),
)
df = df.with_columns(
    ((pl.col("y") >= 2).cast(pl.Int8)).alias("target")
)
df.group_by("target").len()
```




<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 2)</small><table border="1" class="dataframe"><thead><tr><th>target</th><th>len</th></tr><tr><td>i8</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>8337</td></tr><tr><td>1</td><td>1663</td></tr></tbody></table></div>




```python
[
    evaluate_feature(df, feature, 'y')
    for feature in ['y'] + features
]
```




    [array([-3.75224909e+18]),
     array([4935.67950579]),
     array([4823.40828645]),
     array([4946.46681529]),
     array([0.88096144]),
     array([1.8142133])]




```python
f_regression(df.select(features).to_numpy(), df["y"].to_numpy())
```




    (array([4.93567951e+03, 4.82340829e+03, 4.94646682e+03, 8.80961436e-01,
            1.81421330e+00]),
     array([0.        , 0.        , 0.        , 0.34796116, 0.17803416]))




```python
f_classif(df.select(features).to_numpy(), df["y"].to_numpy())
```

    /Users/michal/.python_venvs/uvpy13/lib/python3.13/site-packages/sklearn/feature_selection/_univariate_selection.py:107: RuntimeWarning: invalid value encountered in divide
      msw = sswn / float(dfwn)





    (array([nan, nan, nan, nan, nan]), array([nan, nan, nan, nan, nan]))




```python
f_classif(df.select(features).to_numpy(), df["target"].to_numpy())
```




    (array([1.73154223e+03, 1.78842297e+03, 1.73917650e+03, 3.51022636e-01,
            2.75823180e-01]),
     array([0.        , 0.        , 0.        , 0.55354821, 0.59946366]))




```python
f_regression(df.select(features).to_numpy(), df["target"].to_numpy())
```




    (array([1.73154223e+03, 1.78842297e+03, 1.73917650e+03, 3.51022636e-01,
            2.75823180e-01]),
     array([0.        , 0.        , 0.        , 0.55354821, 0.59946366]))



## redo using the `dataset.make_regression` 


```python
X, y = make_regression(
     n_samples=10000, n_features=5, n_informative=2, noise=1e-4, random_state=42
)
```
